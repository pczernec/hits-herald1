{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Science Summit\n",
    "## Fantastical bugs and where to find them in HRL systems\n",
    "This tutorial is designed to familiarize yourself with the most frequent problems and bugs encountered while developing Reinforcement Learning systems, especially the hierarchical ones.\n",
    "Author: Michał Bortkiewicz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "from typing import TypeVar, Callable, Any, cast, Tuple, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.lib.display import IFrame\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Project files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import ROOT_DIR\n",
    "from scripts.run.train import override_params, create_wandb_config, create_wandb_name\n",
    "from scripts.run.core import get_env_and_graph, load_params, run_session\n",
    "from run.run_from_files import seed_everything"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "## The HRL setup and source code organization\n",
    "[Graph RL](https://github.com/nicoguertler/graph_rl) library impplmeneted by Nico Gürtler et al., used in [Hierarchical Reinforcement Learning with Timed Subgoals](https://proceedings.neurips.cc/paper/2021/file/b59c21a078fde074a6750e91ed19fb21-Paper.pdf) and our tutorial, provides graph abstraction over agent elements that are represented by nodes and are composed of three elements:\n",
    "- Policy\n",
    "- Subtask\n",
    "- Algorithm\n",
    "\n",
    "![graph_rl](graph_and_node.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## The environment\n",
    "We will use [Platforms environment](https://www.youtube.com/watch?v=JkPaI3uZU6c&t=432s&ab_channel=NicoG%C3%BCrtler), introduced by Nico Gürtler et al in [Hierarchical Reinforcement Learning with Timed Subgoals](https://proceedings.neurips.cc/paper/2021/file/b59c21a078fde074a6750e91ed19fb21-Paper.pdf). The goal of the (black) ball in this environment is to reach black circle by crossing the moving platforms. Blue ball represents the subgoals for the agent.\n",
    "![Platforms](platforms.png)\n",
    "For the purposes of this tutorial we will use easier version of Platforms environment (for faster policy convergence) where the final environment goal is located nearer, just above the first platform.\n",
    "![EasyPlatforms](easy_platforms.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running the training and evaluation loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run params and wandb setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Main run parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Environment and Agent\n",
    "env_name = \"EasyPlatforms\"\n",
    "algo_name = \"SAC\"\n",
    "# Additional (overriding the json files) parameters for graph generation and run\n",
    "graph_params = []\n",
    "run_params = []\n",
    "# Paths\n",
    "env_algo_path = os.path.join(f\"{ROOT_DIR}/data\", env_name, algo_name + \"_trained\")\n",
    "wandb_params_path = os.path.join(\n",
    "    env_algo_path, \"wandb_params.json\"\n",
    ")\n",
    "log_dir = os.path.join(env_algo_path, \"log\")\n",
    "# Session\n",
    "step = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create overriding functions and load graph and run parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_params_override = functools.partial(override_params, params=graph_params)\n",
    "run_params_override = functools.partial(override_params, params=run_params)\n",
    "run_params, graph_params, _ = load_params(env_algo_path)\n",
    "if graph_params_override:\n",
    "    graph_params_override(graph_params)\n",
    "if run_params_override:\n",
    "    run_params_override(run_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load wandb parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(wandb_params_path) as json_file:\n",
    "    wandb_params = json.load(json_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create config that can be logged to wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config_to_log = create_wandb_config(\n",
    "    env_algo_path,\n",
    "    override_graph_params=graph_params_override,\n",
    "    override_run_params=run_params_override,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config_to_log"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Init wandb run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wandb login"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wandb online  # Remove this if not debugging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_wandb = wandb.init(\n",
    "    dir=os.path.join(ROOT_DIR),\n",
    "    project=wandb_params[\"project\"],\n",
    "    entity=wandb_params[\"entity\"],\n",
    "    name=create_wandb_name(wandb_params[\"name\"], config_to_log),\n",
    "    group=wandb_params[\"group\"],\n",
    "    tags=wandb_params[\"tags\"] if \"tags\" in wandb_params else [],\n",
    "    sync_tensorboard=wandb_params[\"sync_tensorboard\"],\n",
    "    monitor_gym=wandb_params[\"monitor_gym\"],\n",
    "    config=config_to_log,\n",
    "    save_code=wandb_params[\"save_code\"],\n",
    ")\n",
    "wandb_params[\"run_id\"] = run_wandb.id\n",
    "run_wandb.log_code()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Enironment and Graph setup\n",
    "Seed python, numpy and pytorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = run_params[\"seed\"]\n",
    "seed_everything(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create environment and graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env, graph = get_env_and_graph(run_params, graph_params, wandb_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Environment information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.action_space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Graph information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sess_props = run_session(\n",
    "    env_algo_path,\n",
    "    graph,\n",
    "    env,\n",
    "    run_params,\n",
    "    step,\n",
    "    wandb_params=wandb_params,\n",
    "    run_wandb=run_wandb,\n",
    "    graph_params=graph_params,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(run_wandb.url)\n",
    "IFrame(run_wandb.url, width=\"100%\", height=720)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flat Agent\n",
    "First, we will implement the Flat Agent, that is based on the SAC method. Flat Agent is, understandably, composed of only one node."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bug #1 - wrong alpha in SAC\n",
    "Based on [SpinningUp blog by OpenAI](https://spinningup.openai.com/en/latest/algorithms/sac.html):\n",
    "> SAC trains a stochastic policy with entropy regularization, and explores in an on-policy way. The entropy regularization coefficient $\\alpha$ explicitly controls the explore-exploit tradeoff, with higher $\\alpha$ corresponding to more exploration, and lower $\\alpha$ corresponding to more exploitation. The right coefficient (the one which leads to the stablest / highest-reward learning) may vary from environment to environment, and could require careful tuning.\n",
    "\n",
    "Thus, we should first estimate good enough $\\alpha$ for exploration in flat agent to later examine the benefits of hierarchical reinforcement learning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group_url = f\"https://wandb.ai/piotrczernecki/herald-hits/groups/DSS-flat/workspace?workspace=user-michalbortkiewicz\"\n",
    "print(group_url)\n",
    "IFrame(src=group_url, width=\"100%\", height=720)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To examine the results more closely, we can use the wandb API to retrieve the results and perform further analysis:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_id='obv9hhbo'\n",
    "\n",
    "api = wandb.Api()\n",
    "entity, project = \"piotrczernecki\", \"herald-hits\"  # set to your entity and project\n",
    "runs = api.runs(entity + \"/\" + project)\n",
    "run = api.run(f\"{entity}/{project}/{run_id}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = run.scan_history()\n",
    "rows = []\n",
    "for i, row in enumerate(history):\n",
    "    if \"hac_node_layer_0_algorithm/train/entropy\" in row.keys():\n",
    "        if i > 10000:\n",
    "            rows.append({k: row[k] for k in row.keys() if k in ['global_step', 'hac_node_layer_0_algorithm/train/entropy']})\n",
    "            if row['global_step']>=30000:\n",
    "                break\n",
    "df = pd.DataFrame(rows)\n",
    "df['hac_node_layer_0_algorithm/train/entropy']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(df['global_step'], df['hac_node_layer_0_algorithm/train/entropy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Solution: Start with the most suspicious plot of logged metrics\n",
    "In case of SAC the most insightful plots are *entropy* and *q-value*. But proceed to them only after you verified that loss returns are calculated properly and that loss is correct."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hierarchical Agent\n",
    "Hierarchical agent used in tutorial is composed of two levels i.e. two nodes each corresponding to the policy with different temporal resolution. Main goal is to achieve the same temporal abstraction in the higher level policy that enables the agent to explore environment efficiently and converge faster than the flat agent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bug #2 - weak temporal abstraction due to wrong budget of HL actions\n",
    "In HiTS, there is a budget of maximum number of actions that can be performed during the episode. If the agent runs out of budget, the episode is terminated and additional penalty for using all the possible HL actions is given. Even, though the budget mechanism is not perfect, i.e. there should be no such thing as budget in HRL algorithm, it is crucial to tune the budget differently for every environment to achieve decent performance in HiTS."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "algo_name = \"HiTS\"\n",
    "env_algo_path = os.path.join(f\"{ROOT_DIR}/data\", env_name, algo_name + \"_trained\")\n",
    "run_params, graph_params, _ = load_params(env_algo_path)\n",
    "env, graph = get_env_and_graph(run_params, graph_params, wandb_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for node in graph._nodes:\n",
    "    print(f\"NODE:\\n{node}\\nACTION SPACE:\\n{node.policy.action_space}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group_url = f\"https://wandb.ai/piotrczernecki/herald-hits/groups/DSS-hierarchical-budget/workspace?workspace=user-michalbortkiewicz\"\n",
    "print(group_url)\n",
    "IFrame(src=group_url, width=\"100%\", height=720)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Solution: Set a challenging budget for HL policy\n",
    "Even though the budget is far from a perfect solution that should be abandoned in future works it serves as the most temporal abstraction widening hyperparameter. Thus, it should be challenging for an agent to achieve success in sparse reward environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bug #3 - incorrect reproducibility of experiments due to hidden random number generation or numerical error\n",
    "The bug is quite obvious, however, because of complex logic in hierarchical methods it may be hard to locate. Some algorithms may call the random number generator more often than others. Thus, the runs of almost identical experiment setups may differ significantly, due to chaotic behavior of the RL training algorithm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hidden random number generator calls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "F = TypeVar('F', bound=Callable[..., Any])\n",
    "\n",
    "def reset_seed(func: F) -> F:\n",
    "    def new_func(*args, **kwargs):\n",
    "        st0 = np.random.get_state()\n",
    "        func_output = func(*args, **kwargs)\n",
    "        np.random.set_state(st0)\n",
    "        return func_output\n",
    "    return cast(F, new_func)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def random_outer1() -> Tuple[Union[float, None], float]:\n",
    "    outer = np.random.randn()\n",
    "    return None, outer\n",
    "\n",
    "def random_inner2() -> float:\n",
    "    return np.random.randn()\n",
    "\n",
    "def random_outer2() -> Tuple[float, float]:\n",
    "    inner = random_inner2()\n",
    "    outer = np.random.randn()\n",
    "    return inner, outer\n",
    "\n",
    "@reset_seed\n",
    "def random_inner3() -> float:\n",
    "    return np.random.randn()\n",
    "\n",
    "def random_outer3() -> Tuple[float, float]:\n",
    "    inner = random_inner3()\n",
    "    outer = np.random.randn()\n",
    "    return inner, outer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_outer = {}\n",
    "for i in range(1,4):\n",
    "    results_outer[f\"random_outer{i}\"] = {}\n",
    "    np.random.seed(10)\n",
    "    for j in range(10):\n",
    "        inner, outer = eval(f\"random_outer{i}()\")\n",
    "        results_outer[f\"random_outer{i}\"][j]=outer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results_outer)\n",
    "df[\"r_o1==r_o2\"] = df[\"random_outer1\"]==df[\"random_outer2\"]\n",
    "df[\"r_o1==r_o3\"] = df[\"random_outer1\"]==df[\"random_outer3\"]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Numerical error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group_url = f\"https://wandb.ai/piotrczernecki/herald-hits/groups/22-v1-reproducibility/workspace?workspace=user-michalbortkiewicz\"\n",
    "print(group_url)\n",
    "IFrame(src=group_url, width=\"100%\", height=720)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Solution: To obtain high reproducibility initialize additional random generators for auxiliary functions\n",
    "It is a good practice, to build new methods incrementally. However, whenever new components are added, make sure that they are decoupled from the original method so that both the original and modified methods may be compared for the same seed.\n",
    "\n",
    "In addition, it may happen that the numerical error accumulates in the run, for instance, due to: different hardware, CUDA system, package version etc. As a result it can lead to completely different results for the SAME random seed for identical experiment configurations!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EAT: Emergency action termination for immediate reaction in hierarchical reinforcement learning\n",
    "Together with Jakub Łyskawa, Paweł Wawrzyński, Mateusz Ostaszewski, Artur Grudkowski and Tomasz Trzciński we submitted EAT to AAMAS 2023 conference. Our main contributions are:\n",
    "- We introduce a method, EAT, of monitoring and possibly terminating higher level actions in hierarchical RL. This method allows a hierarchical policy to immediately react to random events in the environment.\n",
    "- We design two strategies for monitoring and terminating the higher level actions.\n",
    "- We introduce a framework for hierarchical decomposition of Markov Decision Processes into subprocesses in which rewards for future events are discounted over time elapsing to their occurrence rather than over the number of actions to their occurrence.\n",
    "\n",
    "EAT introduces higher level action interruption based on the heuristic (performed at every environment step) that rejects current (obsolete) action continuation according to one of two approaches:\n",
    "- *Changing Q*. In this strategy, we terminate the current action if it seems to be worse that the proposed alternative.\n",
    "- *Changing target*. In this strategy, we terminate the current action if it significantly differs from the proposed one.\n",
    "\n",
    "![idea](idea.png)\n",
    "\n",
    "We found out that such action interruption mechanism does not decrease the performance of the hierarchical agent in regular environments. Apparently, EAT significantly improves the success rate in environments with a certain kind of noise, where immediate reaction is necessary to omit emergent obstacles.\n",
    "\n",
    "EAT in action is presented below, in the modified Platforms environment in which every platform might be frozen with a particular probability at every time step.\n",
    "![gif](nb.gif)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group_url = f\"https://wandb.ai/piotrczernecki/herald-hits/groups/AAMAS-q-noisy-platforms/workspace?workspace=user-michalbortkiewicz\"\n",
    "print(group_url)\n",
    "IFrame(src=group_url, width=\"100%\", height=720)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using wandb for ablation studies and further artifacts analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can easily import the model for further analysis or training from wandb using wandb API as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "artifact = api.artifact('piotrczernecki/herald-hits/HAC_obv9hhbo:v0', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "os.listdir(artifact_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = os.path.join(artifact_dir, os.listdir(artifact_dir)[0])\n",
    "graph.load_parameters(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can continue training from the loaded checkpoint."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sess_props = run_session(\n",
    "    env_algo_path,\n",
    "    graph,\n",
    "    env,\n",
    "    run_params,\n",
    "    step,\n",
    "    wandb_params=wandb_params,\n",
    "    run_wandb=run_wandb,\n",
    "    graph_params=graph_params,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
